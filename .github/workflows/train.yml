name: Training Pipeline

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      data_version:
        description: 'DVC data version/tag to use for training'
        required: false
        default: 'latest'
      model_name:
        description: 'MLflow model name'
        required: false
        default: 'code_model_fine_tuning_model'
      experiment_name:
        description: 'MLflow experiment name'
        required: false
        default: 'code_model_training'

env:
  PYTHON_VERSION: "3.10"
  AWS_REGION: "us-west-2"

permissions:
  id-token: write   # This is required for requesting the JWT
  contents: read    # This is required for actions/checkout

jobs:
  training:
    runs-on: ubuntu-latest
    name: Model Training Pipeline
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Required for DVC to work properly
    
    - name: Configure AWS credentials using OIDC
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
        role-session-name: GitHubActions-Training
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: 'requirements.txt'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Cache DVC data
      uses: actions/cache@v3
      with:
        path: |
          .dvc/cache
          data/
        key: dvc-${{ runner.os }}-${{ hashFiles('dvc.lock', '.dvc/config') }}
        restore-keys: |
          dvc-${{ runner.os }}-
    
    - name: Setup DVC
      run: |
        # Configure DVC with AWS S3 remote
        dvc remote modify origin region ${{ env.AWS_REGION }}
        dvc remote modify origin use_ssl true
        # DVC will use AWS credentials from the OIDC configuration
    
    - name: Pull data with DVC
      run: |
        echo "Pulling data from DVC remote..."
        dvc pull
        echo "DVC data pull completed successfully"
    
    - name: Verify data integrity
      run: |
        echo "Verifying data integrity..."
        # Check if required data files exist
        if [ ! -f "data/processed/code_review/train.json" ]; then
          echo "Error: Training data not found"
          exit 1
        fi
        if [ ! -f "data/processed/code_review/val.json" ]; then
          echo "Error: Validation data not found"
          exit 1
        fi
        echo "Data integrity check passed"
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build training Docker image
      run: |
        echo "Building training Docker image..."
        docker build -f docker/train.Dockerfile -t ml-training:${{ github.sha }} .
        echo "Training image built successfully"
    
    - name: Run training in Docker container
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        MLFLOW_EXPERIMENT_NAME: ${{ github.event.inputs.experiment_name || 'code_model_training' }}
        MODEL_NAME: ${{ github.event.inputs.model_name || 'code_model_fine_tuning_model' }}
        AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
      run: |
        echo "Starting training pipeline..."
        
        # Create MLflow tracking directory
        mkdir -p mlflow_artifacts
        
        # Run training container
        docker run --rm \
          -v $(pwd)/data:/home/app/data:ro \
          -v $(pwd)/mlflow_artifacts:/home/app/mlflow_artifacts \
          -e MLFLOW_TRACKING_URI="${MLFLOW_TRACKING_URI}" \
          -e MLFLOW_EXPERIMENT_NAME="${MLFLOW_EXPERIMENT_NAME}" \
          -e MODEL_NAME="${MODEL_NAME}" \
          -e AWS_ACCESS_KEY_ID \
          -e AWS_SECRET_ACCESS_KEY \
          -e AWS_SESSION_TOKEN \
          -e AWS_DEFAULT_REGION \
          ml-training:${{ github.sha }}
        
        echo "Training completed successfully"
    
    - name: Verify MLflow artifacts
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI || 'sqlite:///mlflow.db' }}
        MLFLOW_EXPERIMENT_NAME: ${{ github.event.inputs.experiment_name || 'code_model_training' }}
      run: |
        echo "Verifying MLflow artifacts..."
        python scripts/verify_mlflow_artifacts.py
    
    - name: Register model in MLflow Model Registry
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI || 'sqlite:///mlflow.db' }}
        MODEL_NAME: ${{ github.event.inputs.model_name || 'code_model_fine_tuning_model' }}
        MLFLOW_EXPERIMENT_NAME: ${{ github.event.inputs.experiment_name || 'code_model_training' }}
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF_NAME: ${{ github.ref_name }}
        GITHUB_RUN_ID: ${{ github.run_id }}
        GITHUB_EVENT_HEAD_COMMIT_TIMESTAMP: ${{ github.event.head_commit.timestamp }}
      run: |
        echo "Registering model in MLflow Model Registry..."
        python scripts/register_mlflow_model.py
    
    - name: Upload training artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: training-artifacts-${{ github.sha }}
        path: |
          mlflow_artifacts/
          training_logs.txt
        retention-days: 30
    
    - name: Cleanup
      if: always()
      run: |
        echo "Cleaning up temporary files..."
        docker system prune -f
        rm -rf mlflow_artifacts/
        echo "Cleanup completed"

  # Notification job for training completion
  notify:
    runs-on: ubuntu-latest
    needs: training
    if: always()
    steps:
    - name: Notify training completion
      run: |
        if [ "${{ needs.training.result }}" == "success" ]; then
          echo "✅ Training pipeline completed successfully"
          echo "Model has been registered in MLflow Model Registry"
          echo "Commit SHA: ${{ github.sha }}"
          echo "Branch: ${{ github.ref_name }}"
        else
          echo "❌ Training pipeline failed"
          echo "Check the training logs for details"
          exit 1
        fi
