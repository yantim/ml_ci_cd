version: '3.8'

services:
  # PostgreSQL Database for MLflow
  postgres:
    image: postgres:15-alpine
    container_name: ml-postgres
    environment:
      POSTGRES_DB: mlflow
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow_password
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - ml-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow -d mlflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles:
      - postgres
      - dev
      - full

  # MLflow Tracking Server with PostgreSQL backend
  mlflow:
    image: python:3.10-slim
    container_name: ml-mlflow
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=${MLFLOW_BACKEND_STORE_URI:-sqlite:///mlflow.db}
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=${MLFLOW_DEFAULT_ARTIFACT_ROOT:-/app/artifacts}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=mlflow
      - POSTGRES_USER=mlflow
      - POSTGRES_PASSWORD=mlflow_password
    volumes:
      - mlflow_data:/app
      - mlflow_artifacts:/app/artifacts
    command: >
      bash -c "
        pip install mlflow[extras]==2.11.0 psycopg2-binary &&
        if [ '${USE_POSTGRES:-false}' = 'true' ]; then
          export MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:mlflow_password@postgres:5432/mlflow
        fi &&
        mlflow server
        --backend-store-uri $$MLFLOW_BACKEND_STORE_URI
        --default-artifact-root $$MLFLOW_DEFAULT_ARTIFACT_ROOT
        --host 0.0.0.0
        --port 5000
      "
    networks:
      - ml-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      postgres:
        condition: service_healthy
        required: false
    profiles:
      - dev
      - train
      - serve
      - full

  # Training Container (on-demand)
  train:
    image: ${TRAIN_IMAGE:-localhost/ml-pipeline/train:latest}
    container_name: ml-train
    depends_on:
      - mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - EXPERIMENT_NAME=${EXPERIMENT_NAME:-code_model_training}
      - MODEL_NAME=${MODEL_NAME:-code_model_fine_tuning_model}
      - PYTHONPATH=/home/app
    volumes:
      - ./data:/home/app/data:ro
      - ./config:/home/app/config:ro
      - model_artifacts:/home/app/models
    networks:
      - ml-network
    profiles:
      - train
      - full

  # Model Serving Container
  serve:
    image: ${SERVE_IMAGE:-localhost/ml-pipeline/serve:latest}
    container_name: ml-serve
    depends_on:
      - mlflow
    environment:
      - MODEL_URI=${MODEL_URI:-models:/code_model_fine_tuning_model/latest}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MODEL_PATH=/app/models
      - ARTIFACTS_PATH=/app/artifacts
    volumes:
      - model_artifacts:/app/models
      - mlflow_artifacts:/app/artifacts
    networks:
      - ml-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - serve-internal
      - full

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: ml-nginx
    ports:
      - "80:80"
      - "8080:8080"
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./docker/nginx/conf.d:/etc/nginx/conf.d:ro
    depends_on:
      - serve
      - mlflow
    networks:
      - ml-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - serve
      - full

  # Prometheus for Metrics Collection
  prometheus:
    image: prom/prometheus:latest
    container_name: ml-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - ml-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - monitoring
      - dev
      - full

  # Grafana for Metrics Visualization
  grafana:
    image: grafana/grafana:latest
    container_name: ml-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    networks:
      - ml-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - monitoring
      - dev
      - full

  # Node Exporter for System Metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: ml-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - ml-network
    profiles:
      - monitoring
      - dev
      - full

volumes:
  postgres_data:
    driver: local
  mlflow_data:
    driver: local
  mlflow_artifacts:
    driver: local
  model_artifacts:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  ml-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
